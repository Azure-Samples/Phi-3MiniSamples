# Unlocking Generative AI with Phi-3-mini: A Guide to Inference and Deployment

Discover how Phi-3-mini, a new series of models from Microsoft, enables deployment of Large Language Models (LLMs) on edge devices and IoT devices. Learn how to use Semantic Kernel, Ollama/LlamaEdge, and ONNX Runtime to access and infer Phi-3-mini models, and explore the possibilities of generative AI in various application scenarios.

## Features

inference phi3-mini model in:

* Semantic Kernel
* Ollama
* LlamaEdge
* ONNX Runtime

## Getting Started

### Prerequisites

- macOS/Windows/Liunx
- python 3.10+

### Guideline

please read my blog https://aka.ms/phi3gettingstarted  to run the demo 


## Resources

- Phi-3 Azure blog https://aka.ms/phi3blog-april
- Phi-3 technical report https://aka.ms/phi3-tech-report
- Learn ONNX Runtime Generative AI https://github.com/microsoft/onnxruntime-genai
- Learn  about  Semantic Kernel https://aka.ms/SemanticKernel
- Read Semantic Kernel Cookbook https://aka.ms/SemanticKernelCookBook
- Learn about LlamaEdge https://github.com/LlamaEdge/LlamaEdge
- Learn about Ollama https://ollama.com/
