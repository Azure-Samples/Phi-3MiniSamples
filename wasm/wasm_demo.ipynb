{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] LlamaEdge version: 0.8.3\n",
      "[INFO] Model name: default\n",
      "[INFO] Model alias: default\n",
      "[INFO] Context size: 512\n",
      "[INFO] Prompt template: phi-3-chat\n",
      "[INFO] Number of tokens to predict: 1024\n",
      "[INFO] Number of layers to run on the GPU: 100\n",
      "[INFO] Batch size for prompt processing: 512\n",
      "[INFO] Temperature for sampling: 1\n",
      "[INFO] Top-p sampling (1.0 = disabled): 1\n",
      "[INFO] Penalize repeat sequence of tokens: 1.1\n",
      "[INFO] Presence penalty (0.0 = disabled): 0\n",
      "[INFO] Frequency penalty (0.0 = disabled): 0\n",
      "[INFO] Enable prompt log: false\n",
      "[INFO] Enable plugin log: false\n",
      "[INFO] Socket address: 0.0.0.0:8080\n",
      "[INFO] Wasi-nn-ggml plugin: b2694 (commit 0d56246f)\n",
      "[INFO] LlamaEdge API server listening on http://0.0.0.0:8080\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! wasmedge --dir .:. --nn-preload default:GGML:AUTO:YourMSPhi3MiniLocation llama-api-server.wasm -p phi-3-chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
